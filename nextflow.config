/*
 * -------------------------------------------------
 *  nf-core/scatacseqflow Nextflow config file
 * -------------------------------------------------
 * Default config options for all environments.
 */

// Global default params, used in configs
params {

//////// BELOW IS FOR PREPROCESS PARAMETER ////////

  input_fastq    = false // path to samplesheet.csv that contain fastq file location
  preprocess          = false // choose from 'default', '10xgenomics', and 'biorad'
  support_genome      = false // if true, print supported genomes and exit
  species_latin_name  = 'NULL' // used for find gene symbols if not present in GTF file with bioMart, intentionally set to 'NULL' instead of false if not supplied. "fasle" should also work.
  species_latin_name  = false


//// References: 'default' and '10xgenomics'
  ref_fasta           = false // path to reference fasta file, must be in .gz format; also used by 10xgenomics
  ref_gtf             = false // path to gtf file
  ref_fasta_ucsc      = false // if set, will download ucsc genome: 'hg19', 'hg38', 'mm10', etc.
  ref_fasta_ensembl   = false // if set, will download ensembl genome: 'homo_sapiens', etc.

//// PREPROCESS OPTION: 'default' ////
  split_fastq         = true  // whether or not to split fastq into chunks for more parallel jobs
  ref_bwa_index       = false // path to the bwa index folder
  ref_minimap2_index  = false // path to the minimap2 index folder
  barcode_correction  = 'pheniqs' // correction method: 'pheniqs' or 'naive'; if false to skip correciton
  whitelist_barcode   = 'assets/whitelist_barcodes' // path to whitelist folder
  barcode_regex       = '[^:]*' // if set, will be used for SPLIT_BAM
  read1_adapter       = 'AGATCGGAAGAGC' // for trimming, default to the first 13 bp of Illumina standard adapters
  read2_adapter       = 'AGATCGGAAGAGC'
  mapper              = 'bwa' // choose from 'bwa' (recommended) or 'minimap2'
  filter              = 'both' // choose from false (no bam filtering will be performed), 'improper' (reads with low mapping quality, extreme fragment size(outside ot 38 - 2000bp), etc. will be filtered out), and 'both' ('improper' + mitochondrial reads will be filtered out.)

//// PREPROCESS OPTION: '10xgenomics' ////
  ref_cellranger_index = false // path to cellranger index folder

//// PREPROCESS OPTION: 'biorad' ////
  biorad_genome        = false // prebuilt biorad genomes, can be 'hg19', etc.


//////// BELOW IS FOR ARCHR PARAMETER ////////

//// ARCHR BASIC PARAMETER ////
  input_fragment       = false // path to samplesheet.csv containing fragment file location
  archr_thread         = 4

//// ARCHR GENOME ////
  archr_genome        = false // prebuilt ArchR genome: choose from 'hg19', 'mm10', etc.
  archr_genome_fasta  = false // path to genome fasta used for ArchR
  // archr_gtf           = false // gtf used for ArchR, compared to ref_gtf, which is only for PREPROCESS indexing
  // use one single ref_gtf
  archr_txdb          = false // Bioconductor TxDb name
  archr_org           = false // Bioconductor OrgDb name
  archr_bsgenome      = false // Bioconductor BSgenome name
  archr_blacklist     = false // path to blacklist bed file, used for building ArchR custom genome file

//// ARCHR OTHER PARAMETERS ////
  archr_filter_doublets_ratio = 1.5 // for filtering out doublet, if false, skip doublet filtering.
  archr_scrnaseq            = false // path to RNAseq Seurat object
  archr_scrnaseq_grouplist  = ''

// for plotting peaks in browser tracks:
  marker_peak_geneSymbol    = '' // Example: 'GATA1'
  marker_peak_clusters      = '' // Example: 'C1'
  marker_peak_clusters2     = '' // Exapmle: '03_Late.Eryth'

// for pairwise testing:
  pairwise_test_clusters_1  = '' // Example: 'C1'
  pairwise_test_clusters_2  = '' // Example: 'C2'
  pairwise_test_clusters2_1 = '' // Example: '03_Late.Eryth'
  pairwise_test_clusters2_2 = '' // Example: '16_Pre.B'

  custom_peaks              = '' // Example: 'Encode_K562_GATA1 = "https://www.encodeproject.org/files/ENCFF632NQI/@@download/ENCFF632NQI.bed.gz", Encode_GM12878_CEBPB = "https://www.encodeproject.org/files/ENCFF761MGJ/@@download/ENCFF761MGJ.bed.gz", Encode_K562_Ebf1 = "https://www.encodeproject.org/files/ENCFF868VSY/@@download/ENCFF868VSY.bed.gz", Encode_K562_Pax5 = "https://www.encodeproject.org/files/ENCFF339KUO/@@download/ENCFF339KUO.bed.gz"'
  trajectory_groups         = false // used to predict trajectory, example: '"01_HSC", "08_GMP.Neut", "11_CD14.Mono.1"'

//// Below are for test only ////

  // archr_txdb = '/Users/kaihu/Projects/workflow/test_data/archr/txdb_hg19.sqlite' // Bioconductor TxDb object, needed when archr_custom_genome is "yes"
  // archr_org           = '/Users/kaihu/Projects/workflow/test_data/archr/org_hg19.sqlite' // Bioconductor Org object, needed when archr_custom_genome is "yes"
  // archr_bsgenome      = '/Users/kaihu/Projects/workflow/test_data/archr/bsgenome_hg19.rds' // Bioconductor BSgenome object, when archr_custom_genome is "yes"


  // archr_scrnaseq             = "/home/kh45w/workflow/test_data/archr/scRNA-Hematopoiesis-Granja-2019.rds"
  // archr_scrnaseq             = false
  // archr_scrnaseq_grouplist   = 'cTNK = c("19", "20", "21", "22", "23", "24", "25"), cNonTNK = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12", "13", "15", "16", "17", "18")'
  // archr_scrnaseq_grouplist   = false

  // marker_peak_geneSymbol     = 'GATA1'
  // marker_peak_clusters       = 'C1'
  // marker_peak_clusters2      = '03_Late.Eryth'
  // Above are used for plotting peaks in browser tracks

  // pairwise_test_clusters_1   = 'C1'
  // pairwise_test_clusters_2   = 'C2'
  // pairwise_test_clusters2_1  = '03_Late.Eryth'
  // pairwise_test_clusters2_2  = '16_Pre.B'
  // custom_peaks               = false
  // custom_peaks               = 'Encode_K562_GATA1 = "https://www.encodeproject.org/files/ENCFF632NQI/@@download/ENCFF632NQI.bed.gz", Encode_GM12878_CEBPB = "https://www.encodeproject.org/files/ENCFF761MGJ/@@download/ENCFF761MGJ.bed.gz", Encode_K562_Ebf1 = "https://www.encodeproject.org/files/ENCFF868VSY/@@download/ENCFF868VSY.bed.gz", Encode_K562_Pax5 = "https://www.encodeproject.org/files/ENCFF339KUO/@@download/ENCFF339KUO.bed.gz"'
  // trajectory_groups          = '"01_HSC", "08_GMP.Neut", "11_CD14.Mono.1"'


//////// OTHER PARAMETERS ////////

  // MultiQC options
  multiqc_config             = false
  multiqc_title              = false
  max_multiqc_email_size     = 25.MB

  // Boilerplate options
  outdir                     = './results'
  tracedir                   = "${params.outdir}/pipeline_info"
  publish_dir_mode           = 'symlink'
  email                      = false
  email_on_fail              = false
  plaintext_email            = false
  monochrome_logs            = false
  help                       = false
  validate_params            = true
  show_hidden_params         = false
  schema_ignore_params       = 'genomes,modules'
  enable_conda               = false
  singularity_pull_docker_container = false

  // Config options
  custom_config_version      = 'master'
  custom_config_base         = "https://raw.githubusercontent.com/nf-core/configs/${params.custom_config_version}"
  hostnames                  = [:]
  config_profile_description = false
  config_profile_contact     = false
  config_profile_url         = false
  config_profile_name        = false

  // Max resource options
  // Defaults only, expecting to be overwritten
  max_memory                 = '128.GB'
  max_memory                 = '16.GB'
  max_cpus                   = 16
  max_time                   = '240.h'

}

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'

// Load modules.config for DSL2 module specific options
includeConfig 'conf/modules.config'

// Load nf-core custom profiles from different Institutions
try {
  includeConfig "${params.custom_config_base}/nfcore_custom.config"
} catch (Exception e) {
  System.err.println("WARNING: Could not load nf-core/config profiles: ${params.custom_config_base}/nfcore_custom.config")
}

profiles {
  local {
    params {
      max_memory = 8.GB // restrict the max memory for each task instances in order to leverage the multi-core parallalization of the Mac. (For 16.GB Mac, setting to 8.GB allows up to 2 parallel jobs.)
      archr_thread = 1 // if not setting to 1, ArchR may pop out error like: some threads output empty results.
    }
  }
  debug { process.beforeScript = 'echo $HOSTNAME' }
  conda {
    params.enable_conda    = true
    docker.enabled         = false
    singularity.enabled    = false
    podman.enabled         = false
    shifter.enabled        = false
    charliecloud.enabled   = false
  }
  docker {
    docker.enabled         = true
    // Avoid this error:
    //   WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.
    // Testing this in nf-core after discussion here https://github.com/nf-core/tools/pull/351
    // once this is established and works well, nextflow might implement this behavior as new default.
    // docker.runOptions      = '-u \$(id -u):\$(id -g)'
    docker.runOptions      = '-u \$(id -u):\$(id -g) --rm -v /Users:/Users -v /tmp:/tmp'
    // note that /tmp is for R to create and save R_tempDir/Rtmpxxxx: https://support.microsoft.com/en-us/topic/error-fatal-error-cannot-create-r-tempdir-21ae3934-3d91-f6dd-8752-3b3f7b4dbc40
    singularity.enabled    = false
    podman.enabled         = false
    shifter.enabled        = false
    charliecloud.enabled   = false
  }
  singularity {
    singularity.enabled    = true
    singularity.autoMounts = true
    docker.enabled         = false
    podman.enabled         = false
    shifter.enabled        = false
    charliecloud.enabled   = false
    singularity.runOptions = '--bind /home:/home' // for test in hpcc
  }
  podman {
    podman.enabled         = true
    docker.enabled         = false
    singularity.enabled    = false
    shifter.enabled        = false
    charliecloud.enabled   = false
  }
  shifter {
    shifter.enabled        = true
    docker.enabled         = false
    singularity.enabled    = false
    podman.enabled         = false
    charliecloud.enabled   = false
  }
  charliecloud {
    charliecloud.enabled   = true
    docker.enabled         = false
    singularity.enabled    = false
    podman.enabled         = false
    shifter.enabled        = false
  }
  lsf {
    process {
      executor = 'lsf'
      queue = 'long'
    }
  }
  lsf_ghpcc {
    process {
      executor = 'lsf'
      clusterOptions = '-R "select[rh=8]"'
      // clusterOptions = '-R "select[rh=6]" -n 1' // will be two lines in .command.run automatically
      queue = 'long'
      withLabel:process_low {
        queue = 'short'
        time  = '4h'
      }
      withLabel:process_medium {
        queue = 'short'
        time  = '4h'
      }
      withLabel:process_high {
        queue = 'short'
        time  = '4h'
      }
    }
  }
  test      { includeConfig 'conf/test.config'      }
  test_full { includeConfig 'conf/test_full.config' }
}

// Export these variables to prevent local Python/R libraries from conflicting with those in the container
env {
  PYTHONNOUSERSITE = 1
  R_PROFILE_USER   = "/.Rprofile"
  R_ENVIRON_USER   = "/.Renviron"
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
  enabled = true
  file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
  enabled = true
  file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
  enabled = true
  file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}
dag {
  enabled = true
  file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.svg"
}

manifest {
  name            = 'scATACpipe'
  author          = 'Kai Hu, Haibo, Julie Lihua Zhu'
  homePage        = 'https://github.com/hukai916/scATACpipe'
  description     = 'A Nextflow pipeline for scATAC-seq data analysis.'
  mainScript      = 'main.nf'
  nextflowVersion = '!>=21.06.0-edge'
  version         = '0.1.0dev'
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
  if (type == 'memory') {
    try {
      if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'time') {
    try {
      if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'cpus') {
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}
